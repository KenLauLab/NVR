{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #numpy and scipy for matrix/array/dataframe manipulations \n",
    "import pandas as pd #pd for dataframe stuff\n",
    "from __future__ import division,print_function #updated printing functions\n",
    "import h5py\n",
    "from sklearn.utils import shuffle\n",
    "import nvr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load the full or preprocessed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want the same dataset to go into both algorithms, we should do any kind of preprocessing here, aside from the normalization and transformation. This is because monocle and dpFeature has its own normalization functions. See the NVR tutorial for details about this preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1597L, 13730L)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1counts=pd.read_csv(\"s1_countsRaw.csv\",header=None)\n",
    "s1countsArr=np.asarray(s1counts)\n",
    "hqGenes=nvr.parseNoise(s1countsArr)\n",
    "s1countsArrHq=nvr.mkIndexedArr(s1countsArr,hqGenes)\n",
    "s1countsArrHq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dataset into a pandas DataFrame so that it is compatible with sklearn.utils shuffle function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1hqdf=pd.DataFrame(s1countsArrHq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Randomly sample the source dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions generate random samples of cells in the source dataset. It does so by shuffling the rows, cells in this case, and selecting the first n number of cells based on the sample partition number. This partition number determines how many evenly spaced out random samples will be generated based on cell numbers. The dataset is what we loaded in step 0. These functions are also available in the nvr package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(partitions,dataset,seed):\n",
    "    parts=np.arange(dataset.shape[0]/partitions,dataset.shape[0],dataset.shape[0]/partitions).astype(int)    \n",
    "    subOut={}\n",
    "    for i in range(parts.shape[0]):\n",
    "        subOut[\"{0}cells\".format(parts[i])]=np.asarray(shuffle(dataset,random_state=seed))[0:parts[i],:]\n",
    "    return subOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function simply wraps the subsample function and returns any number of dictionaries based on the number of replicates designated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampleReplicates(repNumber,partitions,dataset,seed):\n",
    "    repOut={}\n",
    "    for i in range(repNumber):\n",
    "        repOut[\"replicate{0}\".format(i)]=subsample(partitions,dataset,seed)\n",
    "    return repOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will make 3 replicate cell number samplings of 10 different granularities, meaning there it will generate random cell samplings representing 10%, 20%, 30%, up to 90% of the source dataset. The pseudorandom seed is set to none so the replicates will actually vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampledDataset=subsampleReplicates(3,10,s1hqdf,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Write the random samples to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few steps will write the data contained by the Python dictionaries to file as a .hdf5, allowing for use in downstream analyses. R also supports this filetype with the h5 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictToFile(dictionary,replicateKey,outFileName):\n",
    "    replicateToFile=h5py.File(outFileName,\"w\")\n",
    "    for i in range(len(dictionary[replicateKey])):\n",
    "        replicateToFile.create_dataset(\"{}\".format(dictionary[replicateKey].keys()[i])\\\n",
    "                                    ,data=dictionary[replicateKey].values()[i]\\\n",
    "                                    ,compression=\"gzip\")\n",
    "    replicateToFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a quick check of the keys used by our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['replicate1', 'replicate0', 'replicate2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampledDataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are used as an argument in the dictToFile function and we repeat it for each replicate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictToFile(sampledDataset,'replicate0',\"rep0-10%parts-s1hq.hdf5\")\n",
    "dictToFile(sampledDataset,'replicate1',\"rep1-10%parts-s1hq.hdf5\")\n",
    "dictToFile(sampledDataset,'replicate2',\"rep2-10%parts-s1hq.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three files, representing the replicate samplings, should now be written to file ready to read into another Python notebook or R environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
